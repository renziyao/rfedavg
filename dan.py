# -*- coding: utf-8 -*-
"""dan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JIBt9KdqKoFaY-6LfQD8XuFPk1cKQxj-
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install dalib

import torch
import torch.nn as nn
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
import torch.optim as optim
import torch.utils.data
import dalib.vision.datasets
from qpsolvers import solve_qp
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class AlexNet(nn.Module):
    def __init__(self):
        super().__init__()
        model = models.alexnet(pretrained=True)
        self.features = model.features
        self.avgpool = model.avgpool
        model = models.alexnet(pretrained=False)
        self.classifier = nn.Sequential(
            model.classifier[0:3],
            model.classifier[3:6],
            model.classifier[6:],
        )
        self.fc = nn.Linear(1000, 31)
        self.fc.weight.data.normal_(0, 0.01)
        self.fc.bias.data.fill_(0.0)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        f1 = self.classifier[0](x)
        f2 = self.classifier[1](f1)
        f3 = self.classifier[2](f2)
        prediction = self.fc(f3)
        return prediction, [f1, f2, f3]


print(AlexNet(), flush=True)

# Office31 dataset generate
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
train_transform = transforms.Compose([
    transforms.Resize((256, 256), 0),
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    normalize,
])
val_transform = transforms.Compose([
    transforms.Resize((256, 256), 0),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    normalize,
])
trainset_source = dalib.vision.datasets.Office31(
    root='./data/',
    task='A',
    transform=train_transform,
)
trainset_target = dalib.vision.datasets.Office31(
    root='./data/',
    task='W',
    transform=train_transform,
)
testset_source = dalib.vision.datasets.Office31(
    root='./data/',
    task='A',
    transform=val_transform,
)
testset_target = dalib.vision.datasets.Office31(
    root='./data/',
    task='W',
    transform=val_transform,
)


class MultipleGaussianKernel(nn.Module):
    def __init__(self, kernel_num=5):
        super().__init__()
        self.kernel_num = kernel_num

    def forward(self, source, target):
        kernel_mul = 2
        kernel_num = self.kernel_num
        n_samples = int(source.size()[0])+int(target.size()[0])
        total = torch.cat([source, target], dim=0)
        total0 = total.unsqueeze(0).expand(
            int(total.size(0)), int(total.size(0)), int(total.size(1)))
        total1 = total.unsqueeze(1).expand(
            int(total.size(0)), int(total.size(0)), int(total.size(1)))
        L2_distance = ((total0-total1)**2).sum(2)
        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)
        bandwidth /= kernel_mul ** (kernel_num // 2)
        bandwidth_list = [bandwidth * (kernel_mul**i)
                          for i in range(kernel_num)]
        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)
                      for bandwidth_temp in bandwidth_list]
        return sum(kernel_val)  # /len(kernel_val)


class MMD(nn.Module):
    def __init__(self, kernel_num=5):
        super().__init__()
        self.kernel_num = kernel_num
        self.mgk = MultipleGaussianKernel(kernel_num)

    def forward(self, x, y):
        batch_size = int(x.size()[0])
        kernels = self.mgk(x, y)

        loss1 = 0
        for s1 in range(batch_size):
            for s2 in range(s1+1, batch_size):
                t1, t2 = s1+batch_size, s2+batch_size
                loss1 += kernels[s1, s2] + kernels[t1, t2]
        loss1 = loss1 / float(batch_size * (batch_size - 1) / 2)

        loss2 = 0
        for s1 in range(batch_size):
            for s2 in range(batch_size):
                t1, t2 = s1+batch_size, s2+batch_size
                loss2 -= kernels[s1, t2] + kernels[s2, t1]
        loss2 = loss2 / float(batch_size * batch_size)
        return loss1 + loss2

		# my version
        # len_x = x.shape[0]
        # len_y = y.shape[0]
        # K = self.mgk(x, y)
        # vec = torch.cat(
        #     (
        #         torch.full((len_x, 1), 1 / len_x),
        #         torch.full((len_y, 1), -1 / len_y)
        #     ),
        #     dim=0,
        # )
        # L = torch.mm(vec, torch.transpose(vec, 0, 1)).to(device)
        # #print('L', L)
        # dis = torch.trace(torch.mm(K, L))
        # #print(dis)
        # return dis


def test_acc(net, testset):
    testloader = torch.utils.data.DataLoader(testset, batch_size=16)
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images = images.to(device)
            labels = labels.to(device)
            outputs, _ = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print('Accuracy of the network on test images: %d %%' % (
        100 * correct / total), flush=True)


class ForeverDataIterator:
    """A data iterator that will never stop producing data"""

    def __init__(self, data_loader: torch.utils.data.DataLoader):
        self.data_loader = data_loader
        self.iter = iter(self.data_loader)

    def __next__(self):
        try:
            data = next(self.iter)
        except StopIteration:
            self.iter = iter(self.data_loader)
            data = next(self.iter)
        return data

    def __len__(self):
        return len(self.data_loader)


trainloader_source = torch.utils.data.DataLoader(
    trainset_source, batch_size=36, drop_last=True, shuffle=True)
trainloader_target = torch.utils.data.DataLoader(
    trainset_target, batch_size=36, drop_last=True, shuffle=True)
net = AlexNet()
net.to(device)
optimizer = optim.SGD([
    {'params': net.features[8:].parameters(), 'lr': 0.0003},
    {'params': net.fc.parameters()},
    {'params': net.classifier.parameters()},
], lr=0.003, momentum=0.9, weight_decay=0.0005, nesterov=True)
criterion = nn.CrossEntropyLoss()
mmd_criterion = MMD()
source_iter = ForeverDataIterator(trainloader_source)
target_iter = ForeverDataIterator(trainloader_target)

meters = {'classifier_loss': [], 'mmd_loss': []}
net.train()
for epoch in range(20000):
	optimizer.zero_grad()
	source_inputs, labels = next(source_iter)
	source_inputs = source_inputs.to(device)
	labels = labels.to(device)
	target_inputs, _ = next(target_iter)
	target_inputs = target_inputs.to(device)
	inputs_all = torch.cat((source_inputs, target_inputs), dim=0)
	outputs, features = net(inputs_all)
	classifier_loss = criterion(
		outputs.narrow(0, 0, source_inputs.shape[0]),
		labels,
	)
	mmd_loss = []
	for item in features:
		mmd_loss.append(mmd_criterion(
			item.narrow(0, 0, source_inputs.shape[0]),
			item.narrow(0, source_inputs.shape[0], source_inputs.shape[0]),
		))
	mmd_loss = sum(mmd_loss)
	tradeoff = 1.0
	loss = classifier_loss + mmd_loss * tradeoff
	loss.backward()
	optimizer.step()
	meters['classifier_loss'].append(classifier_loss.item())
	meters['mmd_loss'].append(mmd_loss.item())
	if epoch % 1000 == 999:
		classifier_loss = sum(meters['classifier_loss']) / len(meters['classifier_loss'])
		mmd_loss = sum(meters['mmd_loss']) / len(meters['mmd_loss'])
		print('[%d] classifier_loss: %.3f, mmd_loss: %.3f' %
				(epoch + 1, classifier_loss, mmd_loss), flush=True)
		meters['classifier_loss'] = []
		meters['mmd_loss'] = []
		net.train(False)
		test_acc(net, testset_source)
		test_acc(net, testset_target)
		net.train(True)
